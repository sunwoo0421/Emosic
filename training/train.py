# training/train.py
import os
import numpy as np
from sklearn.metrics import accuracy_score

# dataset_loader import: ë£¨íŠ¸ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ê±°ë‚˜ -m training.train ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
try:
    from training.dataset_loader import load_and_preprocess
except Exception:
    from dataset_loader import load_and_preprocess

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)
import torch

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {"accuracy": accuracy_score(labels, preds)}

def main():
    print("ğŸš€ Starting training process...")

    # 1) ë°ì´í„° ë¡œë“œ (dataset_loader.pyê°€ DatasetDict ë°˜í™˜í•´ì•¼ í•¨)
    dataset = load_and_preprocess(data_dir="./data")
    print("âœ… Dataset loaded. Splits:", list(dataset.keys()))

    # 2) ëª¨ë¸/í† í¬ë‚˜ì´ì € ì„¤ì • â€” ì†ë„ ìµœì í™” ê¶Œì¥ê°’ ì ìš©
    # ë°”ê¿€ ëª¨ë¸ì„ ì›í•˜ë©´ ì•„ë˜ model_name ë³€ìˆ˜ë§Œ ìˆ˜ì •
    model_name = "monologg/koelectra-small-v3-discriminator"  # <-- ë” ë¹ ë¥¸ small ëª¨ë¸ ê¶Œì¥
    num_labels = 44  # KOTE ë¼ë²¨ ìˆ˜ (ë‹¨ì¼-labelë¡œ í•™ìŠµ ì¤‘)
    print(f"ğŸš€ Loading tokenizer & model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

    # 3) í† í°í™”: max_lengthë¥¼ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ (ê¶Œì¥: 64)
    max_length = 64
    def tokenize_fn(batch):
        # batchingì‹œ tokenizerê°€ ë¦¬ìŠ¤íŠ¸/ë¬¸ìì—´ ì²˜ë¦¬í•˜ë„ë¡ padding/truncation ë³´ì¥
        return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=max_length)

    print("ğŸš€ Tokenizing dataset (max_length=%d)..." % max_length)
    tokenized = dataset.map(tokenize_fn, batched=True)

    # 4) Trainer ì¸ì â€” ì†ë„/ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜
    # GPUê°€ ìˆëŠ” ê²½ìš° fp16ì„ ì¼­ë‹ˆë‹¤. (ì—†ìœ¼ë©´ ë¬´ì‹œ)
    has_cuda = torch.cuda.is_available()
    print("GPU available:", has_cuda)

    # ê¸°ë³¸ ì„¤ì •ê°’ â€” í•„ìš”í•˜ë©´ ë³€ê²½
    per_device_train_batch_size = 16   # GPU ë©”ëª¨ë¦¬ ì—¬ìœ  ì—†ìœ¼ë©´ 8ë¡œ ë‚®ì¶”ê³  gradient_accumulation_steps ëŠ˜ë¦¬ê¸°
    per_device_eval_batch_size = 32
    gradient_accumulation_steps = 1    # effective batch = per_device_train_batch_size * gradient_accumulation_steps

    # TrainingArguments: evaluation/saveë¥¼ epoch ë‹¨ìœ„ë¡œ í•˜ì—¬ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    training_args = TrainingArguments(
        output_dir="./outputs",
        overwrite_output_dir=False,
        do_train=True,
        do_eval=True,
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=32,
        learning_rate=2e-5,
    )
    # 5) Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized.get("validation"),
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
    )

    # 6) (ì„ íƒ) ì²´í¬í¬ì¸íŠ¸/ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ê¶Œí•œ/ìƒì„± í™•ì¸
    os.makedirs(training_args.output_dir, exist_ok=True)

    # 7) í•™ìŠµ ì‹œì‘
    print("ğŸš€ Training start...")
    trainer.train()
    print("âœ… Training finished")

    

if __name__ == "__main__":
    main()
